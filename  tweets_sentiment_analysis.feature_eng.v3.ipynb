{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# <center>Feature Engineering (v3)</center>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<br>\n<br>\n<p>In this last version, we will implement a model using Spark through PySpark. First we'll create a Spark session and read the train and test data files.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\nimport pandas as pd\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('ml_tweet_sentiment').getOrCreate()\ndf = spark.read.csv('training_data.csv', header = True, inferSchema = True)\ndf.printSchema()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- _c0: integer (nullable = true)\n |-- target: integer (nullable = true)\n |-- text: string (nullable = true)\n\n"
                }
            ], 
            "execution_count": 3
        }, 
        {
            "source": "df.show(5)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+------+--------------------+\n|_c0|target|                text|\n+---+------+--------------------+\n|  0|     0|@switchfoot http:...|\n|  1|     0|is upset that he ...|\n|  2|     0|@Kenichan I dived...|\n|  3|     0|my whole body fee...|\n|  4|     0|@nationwideclass ...|\n+---+------+--------------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "execution_count": 4
        }, 
        {
            "source": "ts = spark.read.csv('test_data.csv', header = True, inferSchema = True)\nts.printSchema()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- _c0: integer (nullable = true)\n |-- target: integer (nullable = true)\n |-- text: string (nullable = true)\n\n"
                }
            ], 
            "execution_count": 8
        }, 
        {
            "source": "ts.show(5)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+------+--------------------+\n|_c0|target|                text|\n+---+------+--------------------+\n|  0|     1|@stellargirl I lo...|\n|  1|     1|Reading my kindle...|\n|  2|     1|Ok, first assesme...|\n|  3|     1|@kenburbary You'l...|\n|  4|     1|@mikefish  Fair e...|\n+---+------+--------------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "execution_count": 9
        }, 
        {
            "source": "<br>\n<br>\n<p>We will apply a Tokenizer to separate the words in the texts. Then we'll apply HahingTF and IDF to convert the words into numbers and to assign it statistical measurement values. We will use a Pipeline to run the process through the two datasets.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.ml.feature import  Tokenizer, HashingTF, IDF\nfrom pyspark.ml import Pipeline\n\n\ntok = Tokenizer(inputCol=\"text\", outputCol=\"words\")\ntf = HashingTF(inputCol=\"words\", outputCol=\"tf\", numFeatures=500)\nidf = IDF(inputCol=\"tf\", outputCol=\"features\")\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 5
        }, 
        {
            "source": "feat_pipeline = Pipeline(stages=[tok, tf, idf])\n\nfeat_model = feat_pipeline.fit(df)\nfeatures = feat_model.transform(df)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 7
        }, 
        {
            "source": "\ntest_model = feat_pipeline.fit(ts)\ntest = test_model.transform(ts)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 11
        }, 
        {
            "source": "<br>\n<br>\n<p>OK, the data is ready to feed and test the new model.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}