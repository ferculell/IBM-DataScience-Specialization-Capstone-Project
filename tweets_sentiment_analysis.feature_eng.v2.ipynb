{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# <center>Feature Engineering (v2)</center>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<br>\n<br>\n<p>Before we get started we need to run the following two code blocks containing the previous work done with the data.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!wget -O trainingandtestdata.zip http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\nprint('unziping ...')\n!unzip -o -j trainingandtestdata.zip", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "--2019-05-08 00:10:55--  http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\nResolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\nConnecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:80... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip [following]\n--2019-05-08 00:10:55--  https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\nConnecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 81363704 (78M) [application/zip]\nSaving to: \u2018trainingandtestdata.zip\u2019\n\n100%[======================================>] 81,363,704  12.3MB/s   in 6.6s   \n\n2019-05-08 00:11:02 (11.7 MB/s) - \u2018trainingandtestdata.zip\u2019 saved [81363704/81363704]\n\nunziping ...\nArchive:  trainingandtestdata.zip\n  inflating: testdata.manual.2009.06.14.csv  \n  inflating: training.1600000.processed.noemoticon.csv  \n"
                }
            ], 
            "execution_count": 2
        }, 
        {
            "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndata = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", header=None, encoding='ISO-8859-1')\ntest = pd.read_csv(\"testdata.manual.2009.06.14.csv\", header=None, encoding='ISO-8859-1')\n\n\ndata.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\ntest.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n\n\ndata[\"target\"] = data[\"target\"].replace(4, 1)\ntest[\"target\"] = test[\"target\"].replace(4, 1)\n\n\ndf = data[[\"target\", \"text\"]]\nts = test[[\"target\", \"text\"]]\n\n\nts_bin = ts[ts[\"target\"]!=2]\nts_neut = ts[ts[\"target\"]==2]\n\n\n\n\ndf.to_csv('training_data.csv')\nts_bin.to_csv('test_data.csv')\nts_neut.to_csv('neutral_data.csv')\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 3
        }, 
        {
            "source": "<br>\n<br>\n<p>As the performance of the model implemented before was not convincing, we will try another approach. We will use only the training dataset here too for an effective comparison with version 1. For the new feature engineering we need to segment the tweet texts into words and convert that words into number features. We will use the Keras Preprocessing module now.</p>\n<p>Let's import the libraries we will use.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n"
                }
            ], 
            "execution_count": 4
        }, 
        {
            "source": "<br>\n<br>\n<p>Let's create the Pandas Dataframe and separate the tweets and the labels in two variables.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "df_m = pd.read_csv(\"training_data.csv\")", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 5
        }, 
        {
            "source": "labels = df_m[\"target\"]\ntweets = df_m[\"text\"]\n\nlabels.count()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "1600000"
                    }, 
                    "execution_count": 6, 
                    "metadata": {}
                }
            ], 
            "execution_count": 6
        }, 
        {
            "source": "<br>\n<br>\n<p>First, we will use the <b>Tokenizer API</b> from <i>Keras</i>. This tool makes the tokenization, converting words into an integers index.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "tok = Tokenizer(num_words=10000)\ntok.fit_on_texts(tweets)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 7
        }, 
        {
            "source": "<br>\n<br>\n<p>We'll create and pad the tokens sequences.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "tweets_seq = tok.texts_to_sequences(tweets)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 8
        }, 
        {
            "source": "max_length = 30\n\npadded_tweets = pad_sequences(tweets_seq, maxlen=max_length, padding='post')\n\nprint(padded_tweets[:5])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[[  39  147   56  473  144    4 1221    7 3659   48  828   12 1955   30\n     2   41    9  385    0    0    0    0    0    0    0    0    0    0\n     0    0]\n [   8  818   17  111   69  565  193  536  126 2097    9    6  299  551\n    85    4 2399  149   40  273 1170    0    0    0    0    0    0    0\n     0    0]\n [   1  321  363   11    3 1298 1751    2  935 1164    3  493   37   31\n    12    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0]\n [   5  450  851  504 3036    6   34   71   13 1169    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0]\n [  36   42   24   23   32   19  617  113   62    1   91  217    1   69\n    68    7   32  135   86    0    0    0    0    0    0    0    0    0\n     0    0]]\n"
                }
            ], 
            "execution_count": 9
        }, 
        {
            "source": "<br>\n<br>\n<p>Now we will split the dataset, 80% for train and 20% for test.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "X_train, X_test, y_train, y_test = train_test_split(padded_tweets, labels, test_size=0.2, random_state=2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 6
        }, 
        {
            "source": "<br>\n<br>\n<p>All the required work for the new version is done.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}