{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# <center>Model Training (v1)</center>", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "<br>\n<br>\n<p>As a first step, we need to run the previously developed code.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n!wget -O trainingandtestdata.zip http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\nprint('unziping ...')\n!unzip -o -j trainingandtestdata.zip\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndata = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", header=None, encoding='ISO-8859-1')\ntest = pd.read_csv(\"testdata.manual.2009.06.14.csv\", header=None, encoding='ISO-8859-1')\n\n\ndata.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\ntest.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n\n\ndata[\"target\"] = data[\"target\"].replace(4, 1)\ntest[\"target\"] = test[\"target\"].replace(4, 1)\n\n\ndf = data[[\"target\", \"text\"]]\nts = test[[\"target\", \"text\"]]\n\n\nts_bin = ts[ts[\"target\"]!=2]\nts_neut = ts[ts[\"target\"]==2]\n\n\n\n\ndf.to_csv('training_data.csv')\nts_bin.to_csv('test_data.csv')\nts_neut.to_csv('neutral_data.csv')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 1
        }, 
        {
            "source": "\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import TruncatedSVD\n\n\n\n\ndf_m = pd.read_csv(\"training_data.csv\")\n\n\n\n\nlabels = df_m[\"target\"]\ntweets = df_m[\"text\"]\n\n\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(tweets)\nprint(X.shape)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "(1600000, 684047)\n"
                }
            ], 
            "execution_count": 2
        }, 
        {
            "source": "\nX_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=2)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 3
        }, 
        {
            "source": "scaler = StandardScaler(with_mean=False)\n\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 4
        }, 
        {
            "source": "svd = TruncatedSVD(n_components=100)\nsvd.fit(X_train)\n\n\nX_train_svd = svd.transform(X_train)\nX_test_svd = svd.transform(X_test)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 5
        }, 
        {
            "source": "<br>\n<br>\n<p>Here we will train and test the first model.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\n\nlogReg = LogisticRegression()\nlogReg.fit(X_train_svd, y_train)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)"
                    }, 
                    "execution_count": 6, 
                    "metadata": {}
                }
            ], 
            "execution_count": 6
        }, 
        {
            "source": "predicted = logReg.predict(X_test_svd)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 7
        }, 
        {
            "source": "print(\"Accuracy for Logistic Regression Classifier: \" + str(metrics.accuracy_score(y_test, predicted)*100)+\"%\")", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Accuracy for Logistic Regression Classifier: 51.69625%\n"
                }
            ], 
            "execution_count": 10
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}