{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# <center>Feature Engineering</center>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<br>\n<br>\n<p>Before we get started we need to run the following two code blocks containing the previous work done with the data.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "--2019-05-07 02:29:25--  http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\nResolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\nConnecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:80... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip [following]\n--2019-05-07 02:29:26--  https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\nConnecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 81363704 (78M) [application/zip]\nSaving to: \u2018trainingandtestdata.zip\u2019\n\n100%[======================================>] 81,363,704  4.69MB/s   in 17s    \n\n2019-05-07 02:29:43 (4.45 MB/s) - \u2018trainingandtestdata.zip\u2019 saved [81363704/81363704]\n\nunziping ...\nArchive:  trainingandtestdata.zip\n  inflating: testdata.manual.2009.06.14.csv  \n  inflating: training.1600000.processed.noemoticon.csv  \n"
                }
            ], 
            "source": "!wget -O trainingandtestdata.zip http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\nprint('unziping ...')\n!unzip -o -j trainingandtestdata.zip"
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndata = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", header=None, encoding='ISO-8859-1')\ntest = pd.read_csv(\"testdata.manual.2009.06.14.csv\", header=None, encoding='ISO-8859-1')\n\n\ndata.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\ntest.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n\n\ndata[\"target\"] = data[\"target\"].replace(4, 1)\ntest[\"target\"] = test[\"target\"].replace(4, 1)\n\n\ndf = data[[\"target\", \"text\"]]\nts = test[[\"target\", \"text\"]]\n\n\nts_bin = ts[ts[\"target\"]!=2]\nts_neut = ts[ts[\"target\"]==2]\n\n\n\n\ndf.to_csv('training_data.csv')\nts_bin.to_csv('test_data.csv')\nts_neut.to_csv('neutral_data.csv')\n\n"
        }, 
        {
            "source": "<br>\n<br>\n<p>We will use only the training dataset in this first aproach. For the feature engineering we need to segment the tweet texts into words. Then, we need to convert that words into number features in order to apply it the machine learning process selected. Also, we will split the dataset into train and test data.</p>\n<p>Let's import the libraries we will use.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import TruncatedSVD\n"
        }, 
        {
            "source": "<br>\n<br>\n<p>Let's create the Pandas Dataframe and separate the tweets and the labels in two variables.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df_m = pd.read_csv(\"training_data.csv\")"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 4, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "1600000"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "labels = df_m[\"target\"]\ntweets = df_m[\"text\"]\n\nlabels.count()"
        }, 
        {
            "source": "<br>\n<br>\n<p>First, we will use <b>TfidfVectorizer</b> from <i>Scikit-learn</i>. This tool makes the tokenization, the vectorization and the TF-IDF statistical estimation to the raw text data.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "(1600000, 684047)\n"
                }
            ], 
            "source": "\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(tweets)\nprint(X.shape)\n"
        }, 
        {
            "source": "<br>\n<br>\n<p>Now we will split the dataset, 80% for train and 20% for test.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=2)"
        }, 
        {
            "source": "<br>\n<br>\n<p>At this point, we will make a standadization of the data. First we fit <b>StandardScaler</b> with X_train data and then we apply the transformation to X_train and X_test data. We have to set the parameter <i>with_mean=False</i> because we are working with sparse matrices.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\nscaler = StandardScaler(with_mean=False)\n\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)"
        }, 
        {
            "source": "<br>\n<br>\n<p>Now it's time to make dimensionality reduction. We can't use PCA for this dataset because it don't work with sparse data. The mandatory method for the type of data we are dealing with here is Latent Semantic Analysis (LSA) and we will apply it through <b>TruncatedSVD</b>. We set the parameter <i>n_components=100</i> as recommended in the Scikit-learn documentation. As we did with StandardScaler, we fit with X_train data and then we apply the transformation to X_train and X_test data.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 8, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "TruncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n       random_state=None, tol=0.0)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "\nsvd = TruncatedSVD(n_components=100)\nsvd.fit(X_train)"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "X_train_svd = svd.transform(X_train)\nX_test_svd = svd.transform(X_test)"
        }, 
        {
            "source": "<br>\n<br>\n<p>Now the datasets are ready for the model implementation stage.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}