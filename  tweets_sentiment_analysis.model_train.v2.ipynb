{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# <center>Model Training (v2)</center>", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "<br>\n<br>\n<p>As a first step, we need to run the previously developed code.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!wget -O trainingandtestdata.zip http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\nprint('unziping ...')\n!unzip -o -j trainingandtestdata.zip", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndata = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", header=None, encoding='ISO-8859-1')\ntest = pd.read_csv(\"testdata.manual.2009.06.14.csv\", header=None, encoding='ISO-8859-1')\n\n\ndata.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\ntest.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n\n\ndata[\"target\"] = data[\"target\"].replace(4, 1)\ntest[\"target\"] = test[\"target\"].replace(4, 1)\n\n\ndf = data[[\"target\", \"text\"]]\nts = test[[\"target\", \"text\"]]\n\n\nts_bin = ts[ts[\"target\"]!=2]\nts_neut = ts[ts[\"target\"]==2]\n\n\n\n\ndf.to_csv('training_data.csv')\nts_bin.to_csv('test_data.csv')\nts_neut.to_csv('neutral_data.csv')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 2
        }, 
        {
            "source": "from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\n\n\n\ndf_m = pd.read_csv(\"training_data.csv\")\n\nlabels = df_m[\"target\"]\ntweets = df_m[\"text\"]\n\nlabels.count()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "1600000"
                    }, 
                    "execution_count": 3, 
                    "metadata": {}
                }
            ], 
            "execution_count": 3
        }, 
        {
            "source": "tok = Tokenizer(num_words=10000)\ntok.fit_on_texts(tweets)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 4
        }, 
        {
            "source": "max_length = 30\n\n\ntweets_seq = tok.texts_to_sequences(tweets)\n\npadded_tweets = pad_sequences(tweets_seq, maxlen=max_length, padding='post')\n\nprint(padded_tweets[:5])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[[  39  147   56  473  144    4 1221    7 3659   48  828   12 1955   30\n     2   41    9  385    0    0    0    0    0    0    0    0    0    0\n     0    0]\n [   8  818   17  111   69  565  193  536  126 2097    9    6  299  551\n    85    4 2399  149   40  273 1170    0    0    0    0    0    0    0\n     0    0]\n [   1  321  363   11    3 1298 1751    2  935 1164    3  493   37   31\n    12    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0]\n [   5  450  851  504 3036    6   34   71   13 1169    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0]\n [  36   42   24   23   32   19  617  113   62    1   91  217    1   69\n    68    7   32  135   86    0    0    0    0    0    0    0    0    0\n     0    0]]\n"
                }
            ], 
            "execution_count": 5
        }, 
        {
            "source": "X_train, X_test, y_train, y_test = train_test_split(padded_tweets, labels, test_size=0.2, random_state=2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 7
        }, 
        {
            "source": "from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Flatten, Input\nfrom keras.optimizers import Adam\n\n\n\n\nmodel = Sequential()\nmodel.add(Embedding(10000, 32, input_length=max_length))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\n\nprint(model.summary())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 30, 32)            320000    \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 960)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 961       \n=================================================================\nTotal params: 320,961\nTrainable params: 320,961\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
                }
            ], 
            "execution_count": 8
        }, 
        {
            "source": "<br>\n<br>\n<p>Now we'll train and test our second model.</p>\n<br>\n<br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "model.fit(X_train, y_train, epochs=10, verbose=1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Epoch 1/10\n1280000/1280000 [==============================] - 98s - loss: 0.3833 - acc: 0.8364    \nEpoch 2/10\n1280000/1280000 [==============================] - 96s - loss: 0.3804 - acc: 0.8377    \nEpoch 3/10\n1280000/1280000 [==============================] - 97s - loss: 0.3780 - acc: 0.8392    \nEpoch 4/10\n1280000/1280000 [==============================] - 98s - loss: 0.3763 - acc: 0.8399    \nEpoch 5/10\n1280000/1280000 [==============================] - 99s - loss: 0.3745 - acc: 0.8409    \nEpoch 6/10\n1280000/1280000 [==============================] - 98s - loss: 0.3732 - acc: 0.8416    \nEpoch 7/10\n1280000/1280000 [==============================] - 100s - loss: 0.3720 - acc: 0.8422   \nEpoch 8/10\n1280000/1280000 [==============================] - 101s - loss: 0.3709 - acc: 0.8427   \nEpoch 9/10\n1280000/1280000 [==============================] - 101s - loss: 0.3698 - acc: 0.8434   \nEpoch 10/10\n1280000/1280000 [==============================] - 97s - loss: 0.3690 - acc: 0.8436    \n"
                }, 
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f0a7f7ae550>"
                    }, 
                    "execution_count": 10, 
                    "metadata": {}
                }
            ], 
            "execution_count": 10
        }, 
        {
            "source": "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\nprint(\"\\nAccuracy for Embedding Neural Net Model: \" + str(accuracy*100) + \"%\")", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "317216/320000 [============================>.] - ETA: 0s\nAccuracy for Embedding Neural Net Model: 76.8090625%\n"
                }
            ], 
            "execution_count": 14
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}